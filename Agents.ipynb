{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00767280-b930-47b0-9bfd-c1668e735a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import re\n",
    "import time\n",
    "from uuid import uuid4\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.tools import Tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.utilities import ApifyWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff0ef206-4ccf-44b2-9480-b0228b3cb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBclientId = os.getenv(\"DBclientId\")\n",
    "DBtoken = os.getenv(\"DBtoken\")\n",
    "OpenAIApiKey = os.getenv(\"OPENAI_API_KEY\")\n",
    "PineConeAPIKey = os.getenv(\"PineConeAPI\")\n",
    "ApifyToken = os.getenv(\"ApifyToken\")\n",
    "\n",
    "pc = Pinecone(api_key=PineConeAPIKey)\n",
    "\n",
    "name = \"agent-db-test\"\n",
    "\n",
    "pc.create_index(\n",
    "        name=name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "\n",
    "\n",
    "index = pc.Index(name=name)\n",
    "\n",
    "apify = ApifyWrapper(apify_api_token=ApifyToken)\n",
    "llm_model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OpenAIApiKey)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = PineconeVectorStore(index=index, \n",
    "                                   embedding=embeddings, \n",
    "                                   pinecone_api_key=PineConeAPIKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd292851-9531-4f52-a777-8a17778acccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDocRetrievalTool(url:str, llm):\n",
    "\n",
    "    def extract_code_blocks(html_content):\n",
    "        \"\"\"Extracts all code blocks from HTML.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        \n",
    "        # Extract <code> elements inside <pre> (formatted code blocks)\n",
    "        code_blocks = [code.get_text() for code in soup.find_all(\"code\")]\n",
    "        \n",
    "        return \"\\n\\n\".join(code_blocks) if code_blocks else \"\"\n",
    "\n",
    "    loader = apify.call_actor(\n",
    "        actor_id=\"apify/website-content-crawler\",\n",
    "        run_input={\n",
    "            \"startUrls\": [{\"url\": \"https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model\"}]\n",
    "        },\n",
    "        dataset_mapping_function=lambda item: Document(\n",
    "            page_content=item.get(\"text\", \"\") + \"\\n\\n\" + extract_code_blocks(item.get(\"html\", \"\")),\n",
    "            metadata={\"source\": item[\"url\"]}\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    loaded_doc = loader.load()\n",
    "    \n",
    "    txt_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, \n",
    "                                              chunk_overlap=100,\n",
    "                                              length_function=len,\n",
    "                                              is_separator_regex=False)\n",
    "    \n",
    "    splitted_doc = txt_splitter.split_documents(loaded_doc)\n",
    "\n",
    "    uuids = [str(uuid4()) for _ in range(len(splitted_doc))]\n",
    "\n",
    "    vector_store.add_documents(documents=splitted_doc, ids=uuids)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    retriever_tool = create_retriever_tool(retriever=retriever,\n",
    "                                           name=\"database_retriever_tool\", \n",
    "                                           description=\"Retrieve relevant information from the document database\")\n",
    "\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "619b3a3e-1210-45b0-81e3-a4d54a18e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "db_retrieval_tool = CreateDocRetrievalTool(url=\"https://www.datacamp.com/tutorial/fine-tuning-deepseek-r1-reasoning-model\", \n",
    "                                           llm=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7376f7a3-f236-48bb-b91d-12dd6aa979cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a strict retrieval-based assistant called Eddy. \"\n",
    "        \"You **must** always retrieve the answer using the available tools and **must not** generate or modify the response. \"\n",
    "        \"Return the answer **exactly as retrieved**, word for word, without paraphrasing or summarizing. \"\n",
    "        \"If no answer is found, respond with: 'No matching answer found in the database.' \"\n",
    "        \"Format the answer in Markdown exactly as retrieved.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d0f08b7-89de-44a4-8eaa-9c74f0f4b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_tools = create_tool_calling_agent(llm=llm_model, \n",
    "                                             prompt=bot_template,\n",
    "                                             tools=[db_retrieval_tool])\n",
    "\n",
    "bot_agent = AgentExecutor(agent=agent_with_tools, \n",
    "                          tools=[db_retrieval_tool],\n",
    "                          verbose=True,\n",
    "                          handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9436117c-2a56-45b3-ad3f-2f70eacdddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `database_retriever_tool` with `{'query': 'Steps in Fine-Tuning DeepSeek R1. Include the codes involved.'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAashi Dutt \n",
      "8 min\n",
      "tutorial\n",
      "DeepSeek R1 Demo Project With Gradio and EasyOCR\n",
      "In this DeepSeek-R1 tutorial, you'll learn how to build a math puzzle solver app by integrating DeepSeek-R1 with EasyOCR and Gradio.\n",
      "Aashi Dutt \n",
      "12 min\n",
      "See MoreSee More\n",
      "EnglishEspañolBetaPortuguêsBetaDeutschBetaFrançaisBeta\n",
      "Found an Error?\n",
      "\n",
      "from trl import SFTTrainer from transformers import TrainingArguments from unsloth import is_bfloat16_supported trainer = SFTTrainer( model=model, tokenizer=tokenizer, train_dataset=dataset, dataset_text_field=\"text\", max_seq_length=max_seq_length, dataset_num_proc=2, args=TrainingArguments( per_device_train_batch_size=2, gradient_accumulation_steps=4, # Use num_train_epochs = 1, warmup_ratio for full training runs! warmup_steps=5, max_steps=60, learning_rate=2e-4, fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported(), logging_steps=10, optim=\"adamw_8bit\", weight_decay=0.01, lr_scheduler_type=\"linear\", seed=3407, output_dir=\"outputs\", ), )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "6. Model training\n",
      "Run the following command to start training. \n",
      "trainer_stats = trainer.train()\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "The training process took 44 minutes to finish. The training loss has gradually reduced, which is a good sign of better model performance.\n",
      "You can view the fill model evaluation report on the Weights and bais dash board by loging into theo the website and viewin the project. \n",
      "If you face issues running the above code, please refer to the Fine-tuning DeepSeek R1 (Reasoning Model) Kaggle notebook. \n",
      "7. Model inference after fine-tuning\n",
      "To compare the results, we will ask the fine-tuned model the same question as before to see what has changed.\n",
      "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\" FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference! inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\") outputs = model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1200, use_cache=True, ) response = tokenizer.batch_decode(outputs) print(response[0].split(\"### Response:\")[1]) \n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "This is much better and more accurate. The chain of thought was direct, and the answer was straightforward and in one paragraph. The fine-tuning was successful.\n",
      "<think> Okay, so let's think about this. We have a 61-year-old woman who's been dealing with involuntary urine loss during things like coughing or sneezing, but she's not leaking at night. That suggests she might have some kind of problem with her pelvic floor muscles or maybe her bladder. Now, she's got a gynecological exam and a Q-tip test. Let's break that down. The Q-tip test is usually used to check for urethral obstruction. If it's positive, that means there's something blocking the urethra, like a urethral stricture or something else. Given that she's had a positive Q-tip test, it's likely there's a urethral obstruction. That would mean her urethra is narrow, maybe due to a stricture or some kind of narrowing. So, her bladder can't empty properly during activities like coughing because the urethral obstruction is making it hard. Now, let's think about what happens when her bladder can't empty. If there's a urethral obstruction, the bladder is forced to hold more urine, increasing the residual volume. That's because her bladder doesn't empty completely. So, her residual volume is probably increased. Also, if her bladder can't empty properly, she might have increased detrusor contractions. These contractions are usually stronger to push the urine out. So, we expect her detrusor contractions to be increased. Putting it all together, if she has a urethral obstruction and a positive Q-tip test, we'd expect her cystometry results to show increased residual volume and increased detrusor contractions. That makes sense because of the obstruction and how her bladder is trying to compensate by contracting more. </think> Based on the findings of the gynecological exam and the positive Q-tip test, it is most likely that the cystometry would reveal increased residual volume and increased detrusor contractions. The positive Q-tip test indicates urethral obstruction, which would force the bladder to retain more urine, thereby increasing the residual volume. Additionally, the obstruction can lead to increased detrusor contractions as the bladder tries to compensate by contracting more to expel the urine.<｜end▁of▁sentence｜> \n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "8. Saving the model locally\n",
      "Now, let's save the adopter, full model, and tokenizer locally so that we can use them in other projects.\n",
      "new_model_local = \"DeepSeek-R1-Medical-COT\" model.save_pretrained(new_model_local) tokenizer.save_pretrained(new_model_local) model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "9. Pushing the model to Hugging Face Hub\n",
      "We will also push the adopter, tokenizer, and model to Hugging Face Hub so that the AI community can take advantage of this model by integrating it into their systems.\n",
      "new_model_online = \"kingabzpro/DeepSeek-R1-Medical-COT\" model.push_to_hub(new_model_online) tokenizer.push_to_hub(new_model_online) model.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Source: kingabzpro/DeepSeek-R1-Medical-COT · Hugging Face\n",
      "The next step in your learning journey is to serve and deploy your model to the cloud. You can follow the How to Deploy LLMs with BentoML guide, which provides a step-by-step process for deploying large language models efficiently and cost-effectively using BentoML and tools like vLLM.\n",
      "Alternatively, if you prefer to use the model locally, you can convert it into GGUF format and run it on your machine. For this, check out the Fine-tuning Llama 3.2 and Using It Locally guide, which provides detailed instructions for local usage.\n",
      "Conclusion\n",
      "Things are changing rapidly in the field of AI. The open-source community is now taking over, challenging the dominance of proprietary models that have ruled the AI landscape for the past three years. \n",
      "Open-source large language models (LLMs) are becoming better, faster, and more efficient, making it easier than ever to fine-tune them on lower compute and memory resources.\n",
      "In this tutorial, we explored the DeepSeek R1 reasoning model and learned how to fine-tune its distilled version for medical Q&A tasks. A fine-tuned reasoning model not only enhances performance but also enables its application in critical fields such as medicine, emergency services, and healthcare.\n",
      "To counter the launch of DeepSeek R1, OpenAI has introduced two powerful tools: OpenAI’s o3, a more advanced reasoning model, and OpenAI's Operator AI agent, powered by the new Computer-Using Agent (CUA) model, which can autonomously navigate websites and perform tasks. \n",
      "OpenAI’s O3: Features, O1 Comparison, Release Date & More\n",
      "OpenAI's Operator: Examples, Use Cases, Competition & More\n",
      "Author\n",
      "Abid Ali Awan\n",
      "As a certified data scientist, I am passionate about leveraging cutting-edge technology to create innovative machine learning applications. With a strong background in speech recognition, data analysis and reporting, MLOps, conversational AI, and NLP, I have honed my skills in developing intelligent systems that can make a real impact. In addition to my technical expertise, I am also a skilled communicator with a talent for distilling complex concepts into clear and concise language. As a result, I have become a sought-after blogger on data science, sharing my insights and experiences with a growing community of fellow data professionals. Currently, I am focusing on content creation and editing, working with large language models to develop powerful and engaging content that can help businesses and individuals alike make the most of their data.\n",
      "Topics\n",
      "Artificial Intelligence\n",
      "Large Language Models\n",
      "Abid Ali AwanCertified data scientist, passionate about building ML apps, blogging on data science, and editing.\n",
      "Topics\n",
      "Artificial Intelligence\n",
      "Large Language Models\n",
      "I Tested DeepSeek R1 Lite Preview to See if It's Better Than O1\n",
      "DeepSeek R1: Features, o1 Comparison, Distilled Models & More\n",
      "What Is OpenAI's Reinforcement Fine-Tuning?\n",
      "DeepSeek's Janus Pro: Features, DALL-E 3 Comparison & More\n",
      "DeepSeek V3: A Guide With Demo Project\n",
      "DeepSeek R1 Demo Project With Gradio and EasyOCR\n",
      "Top DataCamp Courses\n",
      "course\n",
      "Fine-Tuning with Llama 3\n",
      "2 hr\n",
      "502\n",
      "Fine-tune Llama for custom tasks using TorchTune, and learn techniques for efficient fine-tuning such as quantization.\n",
      "See Details\n",
      "Start Course\n",
      "course\n",
      "Introduction to LLMs in Python\n",
      "4 hr\n",
      "12.2K\n",
      "Learn the nuts and bolts of LLMs and the revolutionary transformer architecture they are based on!\n",
      "See Details\n",
      "Start Course\n",
      "course\n",
      "Transformer Models with PyTorch\n",
      "2 hr\n",
      "384\n",
      "What makes LLMs tick? Discover how transformers revolutionized text modeling and kickstarted the generative AI boom.\n",
      "See Details\n",
      "Start Course\n",
      "See More\n",
      "Related\n",
      "blog\n",
      "I Tested DeepSeek R1 Lite Preview to See if It's Better Than O1\n",
      "I tested the capabilities of the new DeepSeek-R1-Lite-Preview (DeepThink) model through a series of math, coding, and logic tasks.\n",
      "Dr Ana Rojo-Echeburúa \n",
      "8 min\n",
      "blog\n",
      "DeepSeek R1: Features, o1 Comparison, Distilled Models & More\n",
      "Learn about DeepSeek-R1's key features, development process, distilled models, how to access it, pricing, and how it compares to OpenAI o1.\n",
      "Alex Olteanu \n",
      "8 min\n",
      "blog\n",
      "What Is OpenAI's Reinforcement Fine-Tuning?\n",
      "Learn about OpenAI's reinforcement fine-tuning, a new technique for refining large language models using a reward-driven training loop.\n",
      "Hesam Sheikh Hassani \n",
      "5 min\n",
      "blog\n",
      "DeepSeek's Janus Pro: Features, DALL-E 3 Comparison & More\n",
      "Learn about DeepSeek's new multimodal AI model, Janus-Pro, how to access it, and how it compares to OpenAI's DALL-E 3.\n",
      "Alex Olteanu \n",
      "8 min\n",
      "tutorial\n",
      "DeepSeek V3: A Guide With Demo Project\n",
      "Learn how to build an AI-powered code reviewer assistant using DeepSeek-V3 and Gradio.\n",
      "Aashi Dutt \n",
      "8 min\n",
      "\n",
      "Get 50% off unlimited learning\n",
      "Buy Now \n",
      "Skip to main content\n",
      "Fine-Tuning DeepSeek R1 (Reasoning Model)\n",
      "Fine-tuning the world's first open-source reasoning model on the medical chain of thought dataset to build better AI doctors for the future.\n",
      "Jan 27, 2025 · 12 min read\n",
      "Contents\n",
      "DeepSeek has disrupted the AI landscape, challenging OpenAI's dominance by launching a new series of advanced reasoning models. The best part? These models are completely free to use with no restrictions, making them accessible to everyone. You can view our video tutorial on how to fine-tune DeepSeek below.\n",
      "Fine Tune DeepSeek R1 | Build a Medical Chatbot - YouTube\n",
      "DataCamp\n",
      "182K subscribers\n",
      "Subscribe\n",
      "Subscribed\n",
      "Fine Tune DeepSeek R1 | Build a Medical Chatbot\n",
      "DataCamp\n",
      "Subscribe\n",
      "Subscribed\n",
      "Search\n",
      "Watch later\n",
      "Share\n",
      "Copy link\n",
      "Info\n",
      "Shopping\n",
      "Tap to unmute\n",
      "If playback doesn't begin shortly, try restarting your device.\n",
      "More videos\n",
      "More videos\n",
      "Watch on\n",
      "0:00\n",
      "0:00 / 48:52•Live\n",
      "•\n",
      "In this tutorial, we will fine-tune the DeepSeek-R1-Distill-Llama-8B model on the Medical Chain-of-Thought Dataset from Hugging Face. This distilled DeepSeek-R1 model was created by fine-tuning the Llama 3.1 8B model on the data generated with DeepSeek-R1. It showcases similar reasoning capabilities as the original model.\n",
      "If you are new to LLMs and fine-tuning, I highly recommend you take the Introduction to LLMs in Python course.\n",
      "Image by Author\n",
      "Introducing DeepSeek R1\n",
      "Chinese AI company DeepSeek AI has open-sourced its first-generation reasoning models, DeepSeek-R1 and DeepSeek-R1-Zero, which rival OpenAI's o1 in performance on reasoning tasks like math, coding, and logic. You can read our full guide to DeepSeek R1 to learn more.\n",
      "DeepSeek-R1-Zero\n",
      "DeepSeek-R1-Zero is the first open-source model trained solely with large-scale reinforcement learning (RL) instead of supervised fine-tuning (SFT) as an initial step. This approach enables the model to independently explore chain-of-thought (CoT) reasoning, solve complex problems, and iteratively refine its outputs. However, it comes with challenges such as repetitive reasoning steps, poor readability, and language mixing that can impact its clarity and usability.\n",
      "DeepSeek-R1\n",
      "DeepSeek-R1 was introduced to overcome the limitations of DeepSeek-R1-Zero by incorporating cold-start data before reinforcement learning, providing a strong foundation for reasoning and non-reasoning tasks. \n",
      "This multi-stage training enables the model to achieve state-of-the-art performance, comparable to OpenAI-o1, across math, code, and reasoning benchmarks while improving its output's readability and coherence.\n",
      "DeepSeek Distillation\n",
      "Along with large language models that require extensive computing power and memory to operate, DeepSeek has also introduced distilled models. These smaller, more efficient models have demonstrated that they can still achieve remarkable reasoning performance. \n",
      "Ranging from 1.5B to 70B parameters, these models retain strong reasoning capabilities, with DeepSeek-R1-Distill-Qwen-32B outperforming OpenAI-o1-mini across multiple benchmarks. \n",
      "Smaller models inherit larger models' reasoning patterns, showcasing the distillation process's effectiveness.\n",
      "Source: deepseek-ai/DeepSeek-R1\n",
      "Read the DeepSeek-R1: Features, o1 Comparison, Distilled Models & More blog to learn about its key features, development process, distilled models, access, pricing, and comparison to OpenAI o1.\n",
      "Fine-Tuning DeepSeek R1: Step-by-Step Guide\n",
      "To fine-tune the DeepSeek R1 model, you can follow the steps below: \n",
      "1. Setting up\n",
      "For this project, we are using Kaggle as our Cloud IDE because it provides free access to GPUs, which are often more powerful than those available in Google Colab. To get started, launch a new Kaggle notebook and add your Hugging Face token and Weights & Biases token as secrets.\n",
      "You can add secrets by navigating to the Add-ons tab in the Kaggle notebook interface and selecting the Secrets option. \n",
      "After setting up the secrets, install the unsloth Python package. Unsloth is an open-source framework designed to make fine-tuning large language models (LLMs) 2X faster and more memory-efficient.\n",
      "Read our Unsloth Guide: Optimize and Speed Up LLM Fine-Tuning to learn about Unsloth's key features, various functions, and how to optimize your fine-tuning workflow. \n",
      "%%capture !pip install unsloth !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Log in to the Hugging Face CLI using the Hugging Face API that we securely extracted from Kaggle Secrets. \n",
      "from huggingface_hub import login from kaggle_secrets import UserSecretsClient user_secrets = UserSecretsClient() hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\") login(hf_token)\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Log in to Weights & Biases (wandb) using your API key and create a new project to track the experiments and fine-tuning progress.\n",
      "import wandb wb_token = user_secrets.get_secret(\"wandb\") wandb.login(key=wb_token) run = wandb.init( project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', job_type=\"training\", anonymous=\"allow\" )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "2. Loading the model and tokenizer\n",
      "For this project, we are loading the Unsloth version of DeepSeek-R1-Distill-Llama-8B. Additionally, we will load the model in 4-bit quantization to optimize memory usage and performance.\n",
      "from unsloth import FastLanguageModel max_seq_length = 2048 dtype = None load_in_4bit = True model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\", max_seq_length = max_seq_length, dtype = dtype, load_in_4bit = load_in_4bit, token = hf_token, )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "3. Model inference before fine-tuning\n",
      "To create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation. The prompt will guide the model to think step-by-step and provide a logical, accurate response.\n",
      "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think>{}\"\"\"\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "In this example, we will provide a medical question to the prompt_style, convert it into tokens, and then pass the tokens to the model for response generation. \n",
      "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\" FastLanguageModel.for_inference(model) inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\") outputs = model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1200, use_cache=True, ) response = tokenizer.batch_decode(outputs) print(response[0].split(\"### Response:\")[1])\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Even without fine-tuning, our model successfully generated a chain of thought and provided reasoning before delivering the final answer. The reasoning process is encapsulated within the <think></think> tags.\n",
      "So, why do we still need fine-tuning? The reasoning process, while detailed, was long-winded and not concise. Additionally, the final answer was presented in a bullet-point format, which deviates from the structure and style of the dataset that we want to fine-tune on.\n",
      "\n",
      "<think> Okay, so I have this medical question to answer. Let me try to break it down. The patient is a 61-year-old woman with a history of involuntary urine loss during activities like coughing or sneezing, but she doesn't leak at night. She's had a gynecological exam and a Q-tip test. I need to figure out what cystometry would show regarding her residual volume and detrusor contractions. First, I should recall what I know about urinary incontinence. Involuntary urine loss during activities like coughing or sneezing makes me think of stress urinary incontinence. Stress incontinence typically happens when the urethral sphincter isn't strong enough to resist increased abdominal pressure from activities like coughing, laughing, or sneezing. This usually affects women, especially after childbirth when the pelvic muscles and ligaments are weakened. The Q-tip test is a common diagnostic tool for stress urinary incontinence. The test involves inserting a Q-tip catheter, which is a small balloon catheter, into the urethra. The catheter is connected to a pressure gauge. The patient is asked to cough, and the pressure reading is taken. If the pressure is above normal (like above 100 mmHg), it suggests that the urethral sphincter isn't closing properly, which is a sign of stress incontinence. So, based on the history and the Q-tip test, the diagnosis is likely stress urinary incontinence. Now, moving on to what cystometry would show. Cystometry, also known as a filling cystometry, is a diagnostic procedure where a catheter is inserted into the bladder, and the bladder is filled with a liquid to measure how much it can hold (residual volume) and how it responds to being filled (like during a cough or sneeze). This helps in assessing the capacity and compliance of the bladder. In a patient with stress incontinence, the bladder's capacity might be normal, but the sphincter's function is impaired. So, during the cystometry, the residual volume might be within normal limits because the bladder isn't overfilled. However, when the patient is asked to cough or perform a Valsalva maneuver, the detrusor muscle (the smooth muscle layer of the bladder) might not contract effectively, leading to an increase in intra-abdominal pressure, which might cause leakage. Wait, but detrusor contractions are usually associated with voiding. In stress incontinence, the issue isn't with the detrusor contractions but with the sphincter's inability to prevent leakage. So, during cystometry, the detrusor contractions would be normal because they are part of the normal voiding process. However, the problem is that the sphincter doesn't close properly, leading to leakage. So, putting it all together, the residual volume might be normal, but the detrusor contractions would be normal as well. The key finding would be the impaired sphincter function leading to incontinence, which is typically demonstrated during the Q-tip test and clinical history. Therefore, the cystometry would likely show normal residual volume and normal detrusor contractions, but the underlying issue is the sphincter's inability to prevent leakage. </think> Based on the provided information, the cystometry findings in this 61-year-old woman with stress urinary incontinence would likely demonstrate the following: 1. **Residual Volume**: The residual volume would be within normal limits. This is because the bladder's capacity is typically normal in cases of stress incontinence, where the primary issue lies with the sphincter function rather than the bladder's capacity. 2. **Detrusor Contractions**: The detrusor contractions would also be normal. These contractions are part of the normal voiding process and are not impaired in stress urinary incontinence. The issue is not with the detrusor muscle but with the sphincter's inability to prevent leakage. In summary, the key findings of the cystometry would be normal residual volume and normal detrusor contractions, highlighting the sphincteric defect as the underlying cause of the incontinence.<｜end▁of▁sentence｜>\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "4. Loading and processing the dataset\n",
      "We will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column. \n",
      "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think> {} </think> {}\"\"\"\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Write the Python function that will create a \"text\" column in the dataset, which consists of the train prompt style. Fill the placeholders with questions, chains of text, and answers. \n",
      "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN def formatting_prompts_func(examples): inputs = examples[\"Question\"] cots = examples[\"Complex_CoT\"] outputs = examples[\"Response\"] texts = [] for input, cot, output in zip(inputs, cots, outputs): text = train_prompt_style.format(input, cot, output) + EOS_TOKEN texts.append(text) return { \"text\": texts, }\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "We will load the first 500 samples from the FreedomIntelligence/medical-o1-reasoning-SFT dataset, which is available on the Hugging Face hub. After that, we will map the text column using the formatting_prompts_func function. \n",
      "from datasets import load_dataset dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) dataset = dataset.map(formatting_prompts_func, batched = True,) dataset[\"text\"][0]\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "As we can see, the text column has a system prompt, instructions, chain of thought, and the answer. \n",
      "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<｜end▁of▁sentence｜>\"\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "5. Setting up the model\n",
      "Using the target modules, we will set up the model by adding the low-rank adopter to the model. \n",
      "model = FastLanguageModel.get_peft_model( model, r=16, target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", ], lora_alpha=16, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context random_state=3407, use_rslora=False, loftq_config=None, )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Next, we will set up the training arguments and the trainer by providing the model, tokenizers, dataset, and other important training parameters that will optimize our fine-tuning process.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `database_retriever_tool` with `{'query': 'Steps in Fine-Tuning DeepSeek R1. Include the codes involved.'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAashi Dutt \n",
      "8 min\n",
      "tutorial\n",
      "DeepSeek R1 Demo Project With Gradio and EasyOCR\n",
      "In this DeepSeek-R1 tutorial, you'll learn how to build a math puzzle solver app by integrating DeepSeek-R1 with EasyOCR and Gradio.\n",
      "Aashi Dutt \n",
      "12 min\n",
      "See MoreSee More\n",
      "EnglishEspañolBetaPortuguêsBetaDeutschBetaFrançaisBeta\n",
      "Found an Error?\n",
      "\n",
      "from trl import SFTTrainer from transformers import TrainingArguments from unsloth import is_bfloat16_supported trainer = SFTTrainer( model=model, tokenizer=tokenizer, train_dataset=dataset, dataset_text_field=\"text\", max_seq_length=max_seq_length, dataset_num_proc=2, args=TrainingArguments( per_device_train_batch_size=2, gradient_accumulation_steps=4, # Use num_train_epochs = 1, warmup_ratio for full training runs! warmup_steps=5, max_steps=60, learning_rate=2e-4, fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported(), logging_steps=10, optim=\"adamw_8bit\", weight_decay=0.01, lr_scheduler_type=\"linear\", seed=3407, output_dir=\"outputs\", ), )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "6. Model training\n",
      "Run the following command to start training. \n",
      "trainer_stats = trainer.train()\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "The training process took 44 minutes to finish. The training loss has gradually reduced, which is a good sign of better model performance.\n",
      "You can view the fill model evaluation report on the Weights and bais dash board by loging into theo the website and viewin the project. \n",
      "If you face issues running the above code, please refer to the Fine-tuning DeepSeek R1 (Reasoning Model) Kaggle notebook. \n",
      "7. Model inference after fine-tuning\n",
      "To compare the results, we will ask the fine-tuned model the same question as before to see what has changed.\n",
      "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\" FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference! inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\") outputs = model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1200, use_cache=True, ) response = tokenizer.batch_decode(outputs) print(response[0].split(\"### Response:\")[1]) \n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "This is much better and more accurate. The chain of thought was direct, and the answer was straightforward and in one paragraph. The fine-tuning was successful.\n",
      "<think> Okay, so let's think about this. We have a 61-year-old woman who's been dealing with involuntary urine loss during things like coughing or sneezing, but she's not leaking at night. That suggests she might have some kind of problem with her pelvic floor muscles or maybe her bladder. Now, she's got a gynecological exam and a Q-tip test. Let's break that down. The Q-tip test is usually used to check for urethral obstruction. If it's positive, that means there's something blocking the urethra, like a urethral stricture or something else. Given that she's had a positive Q-tip test, it's likely there's a urethral obstruction. That would mean her urethra is narrow, maybe due to a stricture or some kind of narrowing. So, her bladder can't empty properly during activities like coughing because the urethral obstruction is making it hard. Now, let's think about what happens when her bladder can't empty. If there's a urethral obstruction, the bladder is forced to hold more urine, increasing the residual volume. That's because her bladder doesn't empty completely. So, her residual volume is probably increased. Also, if her bladder can't empty properly, she might have increased detrusor contractions. These contractions are usually stronger to push the urine out. So, we expect her detrusor contractions to be increased. Putting it all together, if she has a urethral obstruction and a positive Q-tip test, we'd expect her cystometry results to show increased residual volume and increased detrusor contractions. That makes sense because of the obstruction and how her bladder is trying to compensate by contracting more. </think> Based on the findings of the gynecological exam and the positive Q-tip test, it is most likely that the cystometry would reveal increased residual volume and increased detrusor contractions. The positive Q-tip test indicates urethral obstruction, which would force the bladder to retain more urine, thereby increasing the residual volume. Additionally, the obstruction can lead to increased detrusor contractions as the bladder tries to compensate by contracting more to expel the urine.<｜end▁of▁sentence｜> \n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "8. Saving the model locally\n",
      "Now, let's save the adopter, full model, and tokenizer locally so that we can use them in other projects.\n",
      "new_model_local = \"DeepSeek-R1-Medical-COT\" model.save_pretrained(new_model_local) tokenizer.save_pretrained(new_model_local) model.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "9. Pushing the model to Hugging Face Hub\n",
      "We will also push the adopter, tokenizer, and model to Hugging Face Hub so that the AI community can take advantage of this model by integrating it into their systems.\n",
      "new_model_online = \"kingabzpro/DeepSeek-R1-Medical-COT\" model.push_to_hub(new_model_online) tokenizer.push_to_hub(new_model_online) model.push_to_hub_merged(new_model_online, tokenizer, save_method = \"merged_16bit\")\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Source: kingabzpro/DeepSeek-R1-Medical-COT · Hugging Face\n",
      "The next step in your learning journey is to serve and deploy your model to the cloud. You can follow the How to Deploy LLMs with BentoML guide, which provides a step-by-step process for deploying large language models efficiently and cost-effectively using BentoML and tools like vLLM.\n",
      "Alternatively, if you prefer to use the model locally, you can convert it into GGUF format and run it on your machine. For this, check out the Fine-tuning Llama 3.2 and Using It Locally guide, which provides detailed instructions for local usage.\n",
      "Conclusion\n",
      "Things are changing rapidly in the field of AI. The open-source community is now taking over, challenging the dominance of proprietary models that have ruled the AI landscape for the past three years. \n",
      "Open-source large language models (LLMs) are becoming better, faster, and more efficient, making it easier than ever to fine-tune them on lower compute and memory resources.\n",
      "In this tutorial, we explored the DeepSeek R1 reasoning model and learned how to fine-tune its distilled version for medical Q&A tasks. A fine-tuned reasoning model not only enhances performance but also enables its application in critical fields such as medicine, emergency services, and healthcare.\n",
      "To counter the launch of DeepSeek R1, OpenAI has introduced two powerful tools: OpenAI’s o3, a more advanced reasoning model, and OpenAI's Operator AI agent, powered by the new Computer-Using Agent (CUA) model, which can autonomously navigate websites and perform tasks. \n",
      "OpenAI’s O3: Features, O1 Comparison, Release Date & More\n",
      "OpenAI's Operator: Examples, Use Cases, Competition & More\n",
      "Author\n",
      "Abid Ali Awan\n",
      "As a certified data scientist, I am passionate about leveraging cutting-edge technology to create innovative machine learning applications. With a strong background in speech recognition, data analysis and reporting, MLOps, conversational AI, and NLP, I have honed my skills in developing intelligent systems that can make a real impact. In addition to my technical expertise, I am also a skilled communicator with a talent for distilling complex concepts into clear and concise language. As a result, I have become a sought-after blogger on data science, sharing my insights and experiences with a growing community of fellow data professionals. Currently, I am focusing on content creation and editing, working with large language models to develop powerful and engaging content that can help businesses and individuals alike make the most of their data.\n",
      "Topics\n",
      "Artificial Intelligence\n",
      "Large Language Models\n",
      "Abid Ali AwanCertified data scientist, passionate about building ML apps, blogging on data science, and editing.\n",
      "Topics\n",
      "Artificial Intelligence\n",
      "Large Language Models\n",
      "I Tested DeepSeek R1 Lite Preview to See if It's Better Than O1\n",
      "DeepSeek R1: Features, o1 Comparison, Distilled Models & More\n",
      "What Is OpenAI's Reinforcement Fine-Tuning?\n",
      "DeepSeek's Janus Pro: Features, DALL-E 3 Comparison & More\n",
      "DeepSeek V3: A Guide With Demo Project\n",
      "DeepSeek R1 Demo Project With Gradio and EasyOCR\n",
      "Top DataCamp Courses\n",
      "course\n",
      "Fine-Tuning with Llama 3\n",
      "2 hr\n",
      "502\n",
      "Fine-tune Llama for custom tasks using TorchTune, and learn techniques for efficient fine-tuning such as quantization.\n",
      "See Details\n",
      "Start Course\n",
      "course\n",
      "Introduction to LLMs in Python\n",
      "4 hr\n",
      "12.2K\n",
      "Learn the nuts and bolts of LLMs and the revolutionary transformer architecture they are based on!\n",
      "See Details\n",
      "Start Course\n",
      "course\n",
      "Transformer Models with PyTorch\n",
      "2 hr\n",
      "384\n",
      "What makes LLMs tick? Discover how transformers revolutionized text modeling and kickstarted the generative AI boom.\n",
      "See Details\n",
      "Start Course\n",
      "See More\n",
      "Related\n",
      "blog\n",
      "I Tested DeepSeek R1 Lite Preview to See if It's Better Than O1\n",
      "I tested the capabilities of the new DeepSeek-R1-Lite-Preview (DeepThink) model through a series of math, coding, and logic tasks.\n",
      "Dr Ana Rojo-Echeburúa \n",
      "8 min\n",
      "blog\n",
      "DeepSeek R1: Features, o1 Comparison, Distilled Models & More\n",
      "Learn about DeepSeek-R1's key features, development process, distilled models, how to access it, pricing, and how it compares to OpenAI o1.\n",
      "Alex Olteanu \n",
      "8 min\n",
      "blog\n",
      "What Is OpenAI's Reinforcement Fine-Tuning?\n",
      "Learn about OpenAI's reinforcement fine-tuning, a new technique for refining large language models using a reward-driven training loop.\n",
      "Hesam Sheikh Hassani \n",
      "5 min\n",
      "blog\n",
      "DeepSeek's Janus Pro: Features, DALL-E 3 Comparison & More\n",
      "Learn about DeepSeek's new multimodal AI model, Janus-Pro, how to access it, and how it compares to OpenAI's DALL-E 3.\n",
      "Alex Olteanu \n",
      "8 min\n",
      "tutorial\n",
      "DeepSeek V3: A Guide With Demo Project\n",
      "Learn how to build an AI-powered code reviewer assistant using DeepSeek-V3 and Gradio.\n",
      "Aashi Dutt \n",
      "8 min\n",
      "\n",
      "Get 50% off unlimited learning\n",
      "Buy Now \n",
      "Skip to main content\n",
      "Fine-Tuning DeepSeek R1 (Reasoning Model)\n",
      "Fine-tuning the world's first open-source reasoning model on the medical chain of thought dataset to build better AI doctors for the future.\n",
      "Jan 27, 2025 · 12 min read\n",
      "Contents\n",
      "DeepSeek has disrupted the AI landscape, challenging OpenAI's dominance by launching a new series of advanced reasoning models. The best part? These models are completely free to use with no restrictions, making them accessible to everyone. You can view our video tutorial on how to fine-tune DeepSeek below.\n",
      "Fine Tune DeepSeek R1 | Build a Medical Chatbot - YouTube\n",
      "DataCamp\n",
      "182K subscribers\n",
      "Subscribe\n",
      "Subscribed\n",
      "Fine Tune DeepSeek R1 | Build a Medical Chatbot\n",
      "DataCamp\n",
      "Subscribe\n",
      "Subscribed\n",
      "Search\n",
      "Watch later\n",
      "Share\n",
      "Copy link\n",
      "Info\n",
      "Shopping\n",
      "Tap to unmute\n",
      "If playback doesn't begin shortly, try restarting your device.\n",
      "More videos\n",
      "More videos\n",
      "Watch on\n",
      "0:00\n",
      "0:00 / 48:52•Live\n",
      "•\n",
      "In this tutorial, we will fine-tune the DeepSeek-R1-Distill-Llama-8B model on the Medical Chain-of-Thought Dataset from Hugging Face. This distilled DeepSeek-R1 model was created by fine-tuning the Llama 3.1 8B model on the data generated with DeepSeek-R1. It showcases similar reasoning capabilities as the original model.\n",
      "If you are new to LLMs and fine-tuning, I highly recommend you take the Introduction to LLMs in Python course.\n",
      "Image by Author\n",
      "Introducing DeepSeek R1\n",
      "Chinese AI company DeepSeek AI has open-sourced its first-generation reasoning models, DeepSeek-R1 and DeepSeek-R1-Zero, which rival OpenAI's o1 in performance on reasoning tasks like math, coding, and logic. You can read our full guide to DeepSeek R1 to learn more.\n",
      "DeepSeek-R1-Zero\n",
      "DeepSeek-R1-Zero is the first open-source model trained solely with large-scale reinforcement learning (RL) instead of supervised fine-tuning (SFT) as an initial step. This approach enables the model to independently explore chain-of-thought (CoT) reasoning, solve complex problems, and iteratively refine its outputs. However, it comes with challenges such as repetitive reasoning steps, poor readability, and language mixing that can impact its clarity and usability.\n",
      "DeepSeek-R1\n",
      "DeepSeek-R1 was introduced to overcome the limitations of DeepSeek-R1-Zero by incorporating cold-start data before reinforcement learning, providing a strong foundation for reasoning and non-reasoning tasks. \n",
      "This multi-stage training enables the model to achieve state-of-the-art performance, comparable to OpenAI-o1, across math, code, and reasoning benchmarks while improving its output's readability and coherence.\n",
      "DeepSeek Distillation\n",
      "Along with large language models that require extensive computing power and memory to operate, DeepSeek has also introduced distilled models. These smaller, more efficient models have demonstrated that they can still achieve remarkable reasoning performance. \n",
      "Ranging from 1.5B to 70B parameters, these models retain strong reasoning capabilities, with DeepSeek-R1-Distill-Qwen-32B outperforming OpenAI-o1-mini across multiple benchmarks. \n",
      "Smaller models inherit larger models' reasoning patterns, showcasing the distillation process's effectiveness.\n",
      "Source: deepseek-ai/DeepSeek-R1\n",
      "Read the DeepSeek-R1: Features, o1 Comparison, Distilled Models & More blog to learn about its key features, development process, distilled models, access, pricing, and comparison to OpenAI o1.\n",
      "Fine-Tuning DeepSeek R1: Step-by-Step Guide\n",
      "To fine-tune the DeepSeek R1 model, you can follow the steps below: \n",
      "1. Setting up\n",
      "For this project, we are using Kaggle as our Cloud IDE because it provides free access to GPUs, which are often more powerful than those available in Google Colab. To get started, launch a new Kaggle notebook and add your Hugging Face token and Weights & Biases token as secrets.\n",
      "You can add secrets by navigating to the Add-ons tab in the Kaggle notebook interface and selecting the Secrets option. \n",
      "After setting up the secrets, install the unsloth Python package. Unsloth is an open-source framework designed to make fine-tuning large language models (LLMs) 2X faster and more memory-efficient.\n",
      "Read our Unsloth Guide: Optimize and Speed Up LLM Fine-Tuning to learn about Unsloth's key features, various functions, and how to optimize your fine-tuning workflow. \n",
      "%%capture !pip install unsloth !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Log in to the Hugging Face CLI using the Hugging Face API that we securely extracted from Kaggle Secrets. \n",
      "from huggingface_hub import login from kaggle_secrets import UserSecretsClient user_secrets = UserSecretsClient() hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\") login(hf_token)\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Log in to Weights & Biases (wandb) using your API key and create a new project to track the experiments and fine-tuning progress.\n",
      "import wandb wb_token = user_secrets.get_secret(\"wandb\") wandb.login(key=wb_token) run = wandb.init( project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', job_type=\"training\", anonymous=\"allow\" )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "2. Loading the model and tokenizer\n",
      "For this project, we are loading the Unsloth version of DeepSeek-R1-Distill-Llama-8B. Additionally, we will load the model in 4-bit quantization to optimize memory usage and performance.\n",
      "from unsloth import FastLanguageModel max_seq_length = 2048 dtype = None load_in_4bit = True model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\", max_seq_length = max_seq_length, dtype = dtype, load_in_4bit = load_in_4bit, token = hf_token, )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "3. Model inference before fine-tuning\n",
      "To create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation. The prompt will guide the model to think step-by-step and provide a logical, accurate response.\n",
      "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think>{}\"\"\"\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "In this example, we will provide a medical question to the prompt_style, convert it into tokens, and then pass the tokens to the model for response generation. \n",
      "question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\" FastLanguageModel.for_inference(model) inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\") outputs = model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1200, use_cache=True, ) response = tokenizer.batch_decode(outputs) print(response[0].split(\"### Response:\")[1])\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Even without fine-tuning, our model successfully generated a chain of thought and provided reasoning before delivering the final answer. The reasoning process is encapsulated within the <think></think> tags.\n",
      "So, why do we still need fine-tuning? The reasoning process, while detailed, was long-winded and not concise. Additionally, the final answer was presented in a bullet-point format, which deviates from the structure and style of the dataset that we want to fine-tune on.\n",
      "\n",
      "<think> Okay, so I have this medical question to answer. Let me try to break it down. The patient is a 61-year-old woman with a history of involuntary urine loss during activities like coughing or sneezing, but she doesn't leak at night. She's had a gynecological exam and a Q-tip test. I need to figure out what cystometry would show regarding her residual volume and detrusor contractions. First, I should recall what I know about urinary incontinence. Involuntary urine loss during activities like coughing or sneezing makes me think of stress urinary incontinence. Stress incontinence typically happens when the urethral sphincter isn't strong enough to resist increased abdominal pressure from activities like coughing, laughing, or sneezing. This usually affects women, especially after childbirth when the pelvic muscles and ligaments are weakened. The Q-tip test is a common diagnostic tool for stress urinary incontinence. The test involves inserting a Q-tip catheter, which is a small balloon catheter, into the urethra. The catheter is connected to a pressure gauge. The patient is asked to cough, and the pressure reading is taken. If the pressure is above normal (like above 100 mmHg), it suggests that the urethral sphincter isn't closing properly, which is a sign of stress incontinence. So, based on the history and the Q-tip test, the diagnosis is likely stress urinary incontinence. Now, moving on to what cystometry would show. Cystometry, also known as a filling cystometry, is a diagnostic procedure where a catheter is inserted into the bladder, and the bladder is filled with a liquid to measure how much it can hold (residual volume) and how it responds to being filled (like during a cough or sneeze). This helps in assessing the capacity and compliance of the bladder. In a patient with stress incontinence, the bladder's capacity might be normal, but the sphincter's function is impaired. So, during the cystometry, the residual volume might be within normal limits because the bladder isn't overfilled. However, when the patient is asked to cough or perform a Valsalva maneuver, the detrusor muscle (the smooth muscle layer of the bladder) might not contract effectively, leading to an increase in intra-abdominal pressure, which might cause leakage. Wait, but detrusor contractions are usually associated with voiding. In stress incontinence, the issue isn't with the detrusor contractions but with the sphincter's inability to prevent leakage. So, during cystometry, the detrusor contractions would be normal because they are part of the normal voiding process. However, the problem is that the sphincter doesn't close properly, leading to leakage. So, putting it all together, the residual volume might be normal, but the detrusor contractions would be normal as well. The key finding would be the impaired sphincter function leading to incontinence, which is typically demonstrated during the Q-tip test and clinical history. Therefore, the cystometry would likely show normal residual volume and normal detrusor contractions, but the underlying issue is the sphincter's inability to prevent leakage. </think> Based on the provided information, the cystometry findings in this 61-year-old woman with stress urinary incontinence would likely demonstrate the following: 1. **Residual Volume**: The residual volume would be within normal limits. This is because the bladder's capacity is typically normal in cases of stress incontinence, where the primary issue lies with the sphincter function rather than the bladder's capacity. 2. **Detrusor Contractions**: The detrusor contractions would also be normal. These contractions are part of the normal voiding process and are not impaired in stress urinary incontinence. The issue is not with the detrusor muscle but with the sphincter's inability to prevent leakage. In summary, the key findings of the cystometry would be normal residual volume and normal detrusor contractions, highlighting the sphincteric defect as the underlying cause of the incontinence.<｜end▁of▁sentence｜>\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "4. Loading and processing the dataset\n",
      "We will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column. \n",
      "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think> {} </think> {}\"\"\"\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Write the Python function that will create a \"text\" column in the dataset, which consists of the train prompt style. Fill the placeholders with questions, chains of text, and answers. \n",
      "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN def formatting_prompts_func(examples): inputs = examples[\"Question\"] cots = examples[\"Complex_CoT\"] outputs = examples[\"Response\"] texts = [] for input, cot, output in zip(inputs, cots, outputs): text = train_prompt_style.format(input, cot, output) + EOS_TOKEN texts.append(text) return { \"text\": texts, }\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "We will load the first 500 samples from the FreedomIntelligence/medical-o1-reasoning-SFT dataset, which is available on the Hugging Face hub. After that, we will map the text column using the formatting_prompts_func function. \n",
      "from datasets import load_dataset dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) dataset = dataset.map(formatting_prompts_func, batched = True,) dataset[\"text\"][0]\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "As we can see, the text column has a system prompt, instructions, chain of thought, and the answer. \n",
      "\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<｜end▁of▁sentence｜>\"\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "5. Setting up the model\n",
      "Using the target modules, we will set up the model by adding the low-rank adopter to the model. \n",
      "model = FastLanguageModel.get_peft_model( model, r=16, target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", ], lora_alpha=16, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context random_state=3407, use_rslora=False, loftq_config=None, )\n",
      "Powered By \n",
      "Was this helpful? Yes No\n",
      "Next, we will set up the training arguments and the trainer by providing the model, tokenizers, dataset, and other important training parameters that will optimize our fine-tuning process.\u001b[0m\u001b[32;1m\u001b[1;3m```markdown\n",
      "To fine-tune the DeepSeek R1 model, you can follow the steps below:\n",
      "\n",
      "1. **Setting up**\n",
      "   For this project, we are using Kaggle as our Cloud IDE because it provides free access to GPUs, which are often more powerful than those available in Google Colab. To get started, launch a new Kaggle notebook and add your Hugging Face token and Weights & Biases token as secrets. You can add secrets by navigating to the Add-ons tab in the Kaggle notebook interface and selecting the Secrets option. After setting up the secrets, install the unsloth Python package. Unsloth is an open-source framework designed to make fine-tuning large language models (LLMs) 2X faster and more memory-efficient.\n",
      "\n",
      "   ```python\n",
      "   %%capture \n",
      "   !pip install unsloth \n",
      "   !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
      "   ```\n",
      "\n",
      "   Log in to the Hugging Face CLI using the Hugging Face API that we securely extracted from Kaggle Secrets.\n",
      "\n",
      "   ```python\n",
      "   from huggingface_hub import login \n",
      "   from kaggle_secrets import UserSecretsClient \n",
      "   user_secrets = UserSecretsClient() \n",
      "   hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\") \n",
      "   login(hf_token)\n",
      "   ```\n",
      "\n",
      "   Log in to Weights & Biases (wandb) using your API key and create a new project to track the experiments and fine-tuning progress.\n",
      "\n",
      "   ```python\n",
      "   import wandb \n",
      "   wb_token = user_secrets.get_secret(\"wandb\") \n",
      "   wandb.login(key=wb_token) \n",
      "   run = wandb.init(project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', job_type=\"training\", anonymous=\"allow\")\n",
      "   ```\n",
      "\n",
      "2. **Loading the model and tokenizer**\n",
      "   For this project, we are loading the Unsloth version of DeepSeek-R1-Distill-Llama-8B. Additionally, we will load the model in 4-bit quantization to optimize memory usage and performance.\n",
      "\n",
      "   ```python\n",
      "   from unsloth import FastLanguageModel \n",
      "   max_seq_length = 2048 \n",
      "   dtype = None \n",
      "   load_in_4bit = True \n",
      "   model, tokenizer = FastLanguageModel.from_pretrained(model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\", max_seq_length=max_seq_length, dtype=dtype, load_in_4bit=load_in_4bit, token=hf_token)\n",
      "   ```\n",
      "\n",
      "3. **Model inference before fine-tuning**\n",
      "   To create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation.\n",
      "\n",
      "   ```python\n",
      "   prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think>{}\"\"\"\n",
      "   ```\n",
      "\n",
      "   In this example, we will provide a medical question to the prompt_style, convert it into tokens, and then pass the tokens to the model for response generation.\n",
      "\n",
      "   ```python\n",
      "   question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\" \n",
      "   FastLanguageModel.for_inference(model) \n",
      "   inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\") \n",
      "   outputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1200, use_cache=True) \n",
      "   response = tokenizer.batch_decode(outputs) \n",
      "   print(response[0].split(\"### Response:\")[1])\n",
      "   ```\n",
      "\n",
      "4. **Loading and processing the dataset**\n",
      "   We will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column.\n",
      "\n",
      "   ```python\n",
      "   train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think> {} </think> {}\"\"\"\n",
      "   ```\n",
      "\n",
      "   Write the Python function that will create a \"text\" column in the dataset, which consists of the train prompt style. Fill the placeholders with questions, chains of text, and answers.\n",
      "\n",
      "   ```python\n",
      "   EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN \n",
      "\n",
      "   def formatting_prompts_func(examples): \n",
      "       inputs = examples[\"Question\"] \n",
      "       cots = examples[\"Complex_CoT\"] \n",
      "       outputs = examples[\"Response\"] \n",
      "       texts = [] \n",
      "       for input, cot, output in zip(inputs, cots, outputs): \n",
      "           text = train_prompt_style.format(input, cot, output) + EOS_TOKEN \n",
      "           texts.append(text) \n",
      "       return { \"text\": texts, }\n",
      "   ```\n",
      "\n",
      "   We will load the first 500 samples from the FreedomIntelligence/medical-o1-reasoning-SFT dataset, which is available on the Hugging Face hub. After that, we will map the text column using the formatting_prompts_func function.\n",
      "\n",
      "   ```python\n",
      "   from datasets import load_dataset \n",
      "   dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split=\"train[0:500]\",trust_remote_code=True) \n",
      "   dataset = dataset.map(formatting_prompts_func, batched=True,) \n",
      "   dataset[\"text\"][0]\n",
      "   ```\n",
      "\n",
      "5. **Setting up the model**\n",
      "   Using the target modules, we will set up the model by adding the low-rank adopter to the model.\n",
      "\n",
      "   ```python\n",
      "   model = FastLanguageModel.get_peft_model(model, r=16, target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], lora_alpha=16, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", random_state=3407, use_rslora=False, loftq_config=None)\n",
      "   ```\n",
      "\n",
      "   Next, we will set up the training arguments and the trainer by providing the model, tokenizers, dataset, and other important training parameters that will optimize our fine-tuning process.\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "To fine-tune the DeepSeek R1 model, you can follow the steps below:\n",
       "\n",
       "1. **Setting up**\n",
       "   For this project, we are using Kaggle as our Cloud IDE because it provides free access to GPUs, which are often more powerful than those available in Google Colab. To get started, launch a new Kaggle notebook and add your Hugging Face token and Weights & Biases token as secrets. You can add secrets by navigating to the Add-ons tab in the Kaggle notebook interface and selecting the Secrets option. After setting up the secrets, install the unsloth Python package. Unsloth is an open-source framework designed to make fine-tuning large language models (LLMs) 2X faster and more memory-efficient.\n",
       "\n",
       "   ```python\n",
       "   %%capture \n",
       "   !pip install unsloth \n",
       "   !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
       "   ```\n",
       "\n",
       "   Log in to the Hugging Face CLI using the Hugging Face API that we securely extracted from Kaggle Secrets.\n",
       "\n",
       "   ```python\n",
       "   from huggingface_hub import login \n",
       "   from kaggle_secrets import UserSecretsClient \n",
       "   user_secrets = UserSecretsClient() \n",
       "   hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\") \n",
       "   login(hf_token)\n",
       "   ```\n",
       "\n",
       "   Log in to Weights & Biases (wandb) using your API key and create a new project to track the experiments and fine-tuning progress.\n",
       "\n",
       "   ```python\n",
       "   import wandb \n",
       "   wb_token = user_secrets.get_secret(\"wandb\") \n",
       "   wandb.login(key=wb_token) \n",
       "   run = wandb.init(project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', job_type=\"training\", anonymous=\"allow\")\n",
       "   ```\n",
       "\n",
       "2. **Loading the model and tokenizer**\n",
       "   For this project, we are loading the Unsloth version of DeepSeek-R1-Distill-Llama-8B. Additionally, we will load the model in 4-bit quantization to optimize memory usage and performance.\n",
       "\n",
       "   ```python\n",
       "   from unsloth import FastLanguageModel \n",
       "   max_seq_length = 2048 \n",
       "   dtype = None \n",
       "   load_in_4bit = True \n",
       "   model, tokenizer = FastLanguageModel.from_pretrained(model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\", max_seq_length=max_seq_length, dtype=dtype, load_in_4bit=load_in_4bit, token=hf_token)\n",
       "   ```\n",
       "\n",
       "3. **Model inference before fine-tuning**\n",
       "   To create a prompt style for the model, we will define a system prompt and include placeholders for the question and response generation.\n",
       "\n",
       "   ```python\n",
       "   prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think>{}\"\"\"\n",
       "   ```\n",
       "\n",
       "   In this example, we will provide a medical question to the prompt_style, convert it into tokens, and then pass the tokens to the model for response generation.\n",
       "\n",
       "   ```python\n",
       "   question = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\" \n",
       "   FastLanguageModel.for_inference(model) \n",
       "   inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\") \n",
       "   outputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=1200, use_cache=True) \n",
       "   response = tokenizer.batch_decode(outputs) \n",
       "   print(response[0].split(\"### Response:\")[1])\n",
       "   ```\n",
       "\n",
       "4. **Loading and processing the dataset**\n",
       "   We will slightly change the prompt style for processing the dataset by adding the third placeholder for the complex chain of thought column.\n",
       "\n",
       "   ```python\n",
       "   train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response. ### Instruction: You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question. ### Question: {} ### Response: <think> {} </think> {}\"\"\"\n",
       "   ```\n",
       "\n",
       "   Write the Python function that will create a \"text\" column in the dataset, which consists of the train prompt style. Fill the placeholders with questions, chains of text, and answers.\n",
       "\n",
       "   ```python\n",
       "   EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN \n",
       "\n",
       "   def formatting_prompts_func(examples): \n",
       "       inputs = examples[\"Question\"] \n",
       "       cots = examples[\"Complex_CoT\"] \n",
       "       outputs = examples[\"Response\"] \n",
       "       texts = [] \n",
       "       for input, cot, output in zip(inputs, cots, outputs): \n",
       "           text = train_prompt_style.format(input, cot, output) + EOS_TOKEN \n",
       "           texts.append(text) \n",
       "       return { \"text\": texts, }\n",
       "   ```\n",
       "\n",
       "   We will load the first 500 samples from the FreedomIntelligence/medical-o1-reasoning-SFT dataset, which is available on the Hugging Face hub. After that, we will map the text column using the formatting_prompts_func function.\n",
       "\n",
       "   ```python\n",
       "   from datasets import load_dataset \n",
       "   dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split=\"train[0:500]\",trust_remote_code=True) \n",
       "   dataset = dataset.map(formatting_prompts_func, batched=True,) \n",
       "   dataset[\"text\"][0]\n",
       "   ```\n",
       "\n",
       "5. **Setting up the model**\n",
       "   Using the target modules, we will set up the model by adding the low-rank adopter to the model.\n",
       "\n",
       "   ```python\n",
       "   model = FastLanguageModel.get_peft_model(model, r=16, target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], lora_alpha=16, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", random_state=3407, use_rslora=False, loftq_config=None)\n",
       "   ```\n",
       "\n",
       "   Next, we will set up the training arguments and the trainer by providing the model, tokenizers, dataset, and other important training parameters that will optimize our fine-tuning process.\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(bot_agent.invoke({\"input\": \"Steps in Fine-Tuning DeepSeek R1. Include the codes involved.\"})[\"output\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP_ENV]",
   "language": "python",
   "name": "conda-env-NLP_ENV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
